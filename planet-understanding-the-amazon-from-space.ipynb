{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom os import listdir\nfrom numpy import zeros\nfrom numpy import asarray\nfrom pandas import read_csv\nfrom PIL import Image\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras import backend\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\nfrom tqdm import tqdm\nimport cv2\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_classes_df = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\ntrain_classes_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df = pd.read_csv('../input/planets-dataset/planet/planet/sample_submission.csv')\nsample_submission_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import io\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_number =10\nimg = io.imread('../input/planets-dataset/planet/planet/train-jpg/train_{}.jpg'.format(image_number))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_labels = set()\ndef append_labels(tags):\n    for tag in tags.split():\n        unique_labels.add(tag)\n\ntrain_classes = train_classes_df.copy()\ntrain_classes['tags'].apply(append_labels)\nunique_labels = list(unique_labels)\nprint(unique_labels)\nlen(unique_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's do one hot encoding (vectorize) the labels in 'train_classes'\nfor tag in unique_labels:\n    train_classes[tag] = train_classes['tags'].apply(lambda x: 1 if tag in x.split() else 0)\n    \n# adding '.jpg' extension to 'image_name'\ntrain_classes['image_name'] = train_classes['image_name'].apply(lambda x: '{}.jpg'.format(x)) \ntrain_classes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col = list(train_classes.columns[2:]) # storing the tags column names as a variable\n\n# initializing an image generator with some data augumentation\nimage_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n\n# loading images from dataframe\nX = image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='/kaggle/input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=1, batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.preprocessing.image.ImageDataGenerator.flow_from_dataframe?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X is an iterable, It contains 317 batches, each batch contains 128 images and labels because \n#40479 / 128 is 316 remainder 31 each image is of shape (128, 128, 3), each label is of shape (17, )\n\n# let's abitrarily view an image\nx109 = X[0][0][109] # first batch, images, 109th image\ny109 = X[0][1][109] # first batch, labels, 109th label\nprint(\"each image's shape is {}\".format(x109.shape))\nprint(\"each label's shape is {}\".format(y109.shape))\nprint('we have {} batches'.format(len(X)))\nprint('each batch has {} images/labels'.format(X[0][0].shape[0]))\nprint('40479/128 is {:.2F}, so the last batch will have {} images/labels'.format(40479/128, X[316][0].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fbeta(ytrue , ypred, beta=2, epsilon=1e-4):\n    beta_squarred = beta**2\n\n    ytrue = tf.cast(ytrue, tf.float32)\n    ypred = tf.cast(tf.greater(tf.cast(ypred, tf.float32), tf.constant(0.5)), tf.float32)\n        \n    tp = tf.reduce_sum(ytrue * ypred, axis=1)\n    fp = tf.reduce_sum(ypred, axis=1) - tp\n    fn = tf.reduce_sum(ytrue, axis=1) - tp\n    \n    precision = tp/(tp+fp+epsilon)\n    recall = tp/(tp+fn+epsilon)\n    \n    fb = (1+beta_squarred)*precision*recall / (beta_squarred*precision + recall + epsilon)\n    return fb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multi_label_acc(ytrue , ypred, epsilon=1e-4):\n    \n    ytrue = tf.cast(ytrue, tf.float32)\n    ypred = tf.cast(tf.greater(tf.cast(ypred, tf.float32), tf.constant(0.5)), tf.float32)\n    \n    tp = tf.reduce_sum(ytrue * ypred, axis=1)\n    fp = tf.reduce_sum(ypred, axis=1) - tp\n    fn = tf.reduce_sum(ytrue, axis=1) - tp\n    \n    ytrue = tf.cast(ytrue, tf.bool)\n    ypred = tf.cast(ypred, tf.bool)\n    \n    tn = tf.reduce_sum(tf.cast(tf.logical_not(ytrue), tf.float32) * tf.cast(tf.logical_not(ypred), tf.float32),\\\n                       axis=1)\n    \n    return (tp+tn)/(tp+tn+fp+fn+epsilon)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model():    \n    planet_model = Sequential()\n    planet_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n                 input_shape=(128,128, 3)))\n    planet_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n    planet_model.add(MaxPooling2D((2, 2)))\n    planet_model.add(Dropout(0.2))\n    planet_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    planet_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    planet_model.add(MaxPooling2D((2, 2)))\n    planet_model.add(Dropout(0.2))\n    planet_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    planet_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    planet_model.add(MaxPooling2D((2, 2)))\n    planet_model.add(Dropout(0.2))\n    planet_model.add(Flatten())\n    planet_model.add(Dense(128, activation='relu'))\n    planet_model.add(Dropout(0.5))\n    planet_model.add(Dense(17, activation='sigmoid'))\n\n    planet_model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[multi_label_acc, fbeta])\n    return planet_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255,validation_split=0.2, horizontal_flip=True)\n\n# generating the 80% training image data\ntrain_gen = train_image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='../input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=0, batch_size=128, subset='training')\n\n# generating the 20% validation image data\nval_gen = train_image_gen.flow_from_dataframe(dataframe=train_classes, \\\n        directory='../input/planets-dataset/planet/planet/train-jpg/', x_col='image_name', y_col=y_col, \\\n       target_size=(128, 128), class_mode='raw', seed=0, batch_size=128, subset='validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setting step size for training and validation image data\nstep_train_size = int(np.ceil(train_gen.samples / train_gen.batch_size))\nstep_val_size = int(np.ceil(val_gen.samples / train_gen.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"first_model = build_model() # building a sequential model for training\n\n# fitting the model\nfirst_model.fit(x=train_gen, steps_per_epoch=step_train_size, validation_data=val_gen, validation_steps=step_val_size,\n         epochs=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import gc\n# del X,y,create_file_mapping,load_dataset,one_hot_encode,create_tag_mapping\n# gc.collect()\nsample_submission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding .jpg extension to 'image_name' in sample_submission data\nsample_submission = sample_submission_df.copy()\nsample_submission['image_name'] = sample_submission['image_name'].apply(lambda x: '{}.jpg'.format(x))\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting the first 40669 'image_name'(s) from the submission_sample dataframe to generate image data from \n# test.jpg folder\ntest1_df = sample_submission.iloc[:40669]['image_name'].reset_index().drop('index', axis=1)\ntest1_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing an image data generator object for the first 40669 images in the sample submission dataframe\ntest_image_gen1 = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n\n# generating the image data for the first 40669 images in the sample submission dataframe\ntest_gen1 = test_image_gen1.flow_from_dataframe(dataframe=test1_df, \\\n            directory='../input/planets-dataset/planet/planet/test-jpg/', x_col='image_name', y_col=None, \\\n            batch_size=128, shuffle=False, class_mode=None, target_size=(128, 128))\n\n# setting the step size for the testing set for the first 40669 images in the sample submission dataframe\nstep_test_size1 = int(np.ceil(test_gen1.samples / test_gen1.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_gen1.reset() # reseting the generator to be sure of avoiding shuffling\npred1 = first_model.predict(test_gen1, steps=step_test_size1, verbose=1) # predicts the first 40669 images in the \n                                                                    # sample submission dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file_names1 = test_gen1.filenames # storing the filenames (images names) of the first 40669 images names in \n                                       # the sample submission dataframe as ordered in the prediction as a \n                                       # variable\n        \n# converting the predictions of the first 40669 to tag names\npred_tags1 = pd.DataFrame(pred1)\npred_tags1 = pred_tags1.apply(lambda x: ' '.join(np.array(unique_labels)[x > 0.5]), axis=1)\n\n# converting the predictions of the first 40669 to a dataframe\nresult1 = pd.DataFrame({'image_name': test_file_names1, 'tags': pred_tags1})\nresult1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selecting the remaining 'image_name'(s) from the submission_sample dataframe to generate image data from \n# test-additional.jpg folder\ntest2_df = sample_submission.iloc[40669:]['image_name'].reset_index().drop('index', axis=1)\ntest2_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initializing an image data generator object for the remaining images in the sample submission dataframe\ntest_image_gen2 = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n\n# generating the image data for the remaining images in the sample submission dataframe\ntest_gen2 = test_image_gen2.flow_from_dataframe(dataframe=test2_df, \\\n            directory='../input/planets-dataset/test-jpg-additional/test-jpg-additional/', x_col='image_name', \\\n            y_col=None, batch_size=128, shuffle=False, class_mode=None, target_size=(128, 128))\n\n# setting the step size for the testing set for the remaining images in the sample submission dataframe\nstep_test_size2 = int(np.ceil(test_gen2.samples / test_gen2.batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_gen2.reset() # reseting the generator to be sure of avoiding shuffling\npred2 = first_model.predict(test_gen2, steps=step_test_size2, verbose=1) # predicts the remaining images in the \n                                                                    # sample submission dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_file_names2 = test_gen2.filenames # storing the filenames (images names) of the remaining images names in \n                                       # the sample submission dataframe as ordered in the prediction as a \n                                       # variable\n        \n# converting the predictions of the remaining images to tag names\npred_tags2 = pd.DataFrame(pred2)\npred_tags2 = pred_tags2.apply(lambda x: ' '.join(np.array(unique_labels)[x > 0.5]), axis=1)\n\n# converting the predictions of the remaining to a dataframe\nresult2 = pd.DataFrame({'image_name': test_file_names2, 'tags': pred_tags2})\nresult2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result = pd.concat([result1, result2]) # concatenate the predictions of the test.jpg and \n                                             # test-additional.jpg into a single dataframe\n    \nfinal_result = final_result.reset_index().drop('index', axis=1) # reseting the index of the dataframe so it \n                                                                # matches that of sample submission datafarme\n\nprint(final_result.shape)\nfinal_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing the .jpg extension from 'iamge_name' column\nfinal_result['image_name'] = final_result['image_name'].apply(lambda x: x[:-4])\nfinal_result.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_result.to_csv('third_submission.csv', index=False) # saving the predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}